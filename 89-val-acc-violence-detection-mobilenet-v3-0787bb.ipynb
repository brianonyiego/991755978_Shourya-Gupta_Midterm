{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381},{"sourceId":4718786,"sourceType":"datasetVersion","datasetId":2730182},{"sourceId":10348202,"sourceType":"datasetVersion","datasetId":6407897}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import layers, models\nimport cv2\nimport os\nimport numpy as np\nfrom tensorflow.keras.callbacks import EarlyStopping , ReduceLROnPlateau\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import Input\nprint('DONE')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:45:14.688421Z","iopub.execute_input":"2025-01-01T16:45:14.688782Z","iopub.status.idle":"2025-01-01T16:45:17.741100Z","shell.execute_reply.started":"2025-01-01T16:45:14.688753Z","shell.execute_reply":"2025-01-01T16:45:17.740017Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:45:26.060364Z","iopub.execute_input":"2025-01-01T16:45:26.061326Z","iopub.status.idle":"2025-01-01T16:45:34.217084Z","shell.execute_reply.started":"2025-01-01T16:45:26.061289Z","shell.execute_reply":"2025-01-01T16:45:34.215916Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: ultralytics in /opt/conda/lib/python3.10/site-packages (8.3.56)\nRequirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (10.3.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.14.1)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.4.0)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.19.0)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.0.13)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import cv2\nimport os\nimport torch\nfrom tqdm import tqdm\nfrom torchvision import models, transforms\nfrom ultralytics import YOLO\nimport random\n\n# Load YOLO model\nyolo_model_path = \"/kaggle/input/yolov5npt/yolov5su.pt\"\nyolo_model = YOLO(yolo_model_path)\n\n# Load a pre-trained CNN (ResNet50) for feature extraction\ncnn_model = models.resnet50(pretrained=True)\ncnn_model.eval()\n\n# Image preprocessing pipeline for CNN input\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Output paths for extracted frames\noutput_paths = {\n    \"train_violence\": \"/kaggle/working/extracted_frames/train/violence\",\n    \"train_nonviolence\": \"/kaggle/working/extracted_frames/train/nonviolence\",\n    \"val_violence\": \"/kaggle/working/extracted_frames/val/violence\",\n    \"val_nonviolence\": \"/kaggle/working/extracted_frames/val/nonviolence\"\n}\n\n# Create output directories if they don't exist\nfor path in output_paths.values():\n    os.makedirs(path, exist_ok=True)\n\n# Function to extract YOLO-detected bounding box regions\ndef extract_yolo_frames(video_path, output_folder, target_fps=1):\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error opening video file: {video_path}\")\n        return\n\n    original_fps = cap.get(cv2.CAP_PROP_FPS)\n    if original_fps == 0:\n        print(f\"Error reading FPS for video: {video_path}\")\n        return\n\n    frame_interval = int(original_fps / target_fps)\n    frame_count, saved_frame_count = 0, 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_interval == 0:\n            # YOLO detection\n            results = yolo_model(frame)\n            for bbox in results[0].boxes.data.tolist():\n                x1, y1, x2, y2, score, class_id = bbox\n                if score > 0.5:  # Confidence threshold\n                    cropped_frame = frame[int(y1):int(y2), int(x1):int(x2)]\n                    if cropped_frame.size > 0:\n                        # Save cropped region\n                        crop_filename = f\"{os.path.splitext(os.path.basename(video_path))[0]}_crop{saved_frame_count}.jpg\"\n                        cv2.imwrite(os.path.join(output_folder, crop_filename), cropped_frame)\n                        saved_frame_count += 1\n        frame_count += 1\n    cap.release()\n\n# Function to extract features from cropped images\ndef extract_features_from_images(image_folder):\n    features = {}\n    for img_file in tqdm(os.listdir(image_folder), desc=\"Extracting CNN features\"):\n        img_path = os.path.join(image_folder, img_file)\n        img = cv2.imread(img_path)\n        if img is None:\n            continue\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n        with torch.no_grad():\n            feature_vector = cnn_model(img_tensor)\n        features[img_file] = feature_vector.squeeze().numpy()  # Store feature vector\n    return features\n\n# Process videos to extract YOLO bounding boxes\ndef process_videos(video_paths, train_output, val_output, split_for_val=False, target_fps=1):\n    for folder in video_paths:\n        video_files = [f for f in os.listdir(folder) if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n        if split_for_val:\n            random.shuffle(video_files)\n            val_count = max(1, int(len(video_files) * 0.1))  # Take 10% for validation\n            val_videos, train_videos = video_files[:val_count], video_files[val_count:]\n        else:\n            val_videos, train_videos = [], video_files\n\n        # Extract YOLO bounding boxes for validation videos\n        for video_file in tqdm(val_videos, desc=f\"Processing validation videos from {folder}\"):\n            extract_yolo_frames(os.path.join(folder, video_file), val_output, target_fps=target_fps)\n\n        # Extract YOLO bounding boxes for training videos\n        for video_file in tqdm(train_videos, desc=f\"Processing training videos from {folder}\"):\n            extract_yolo_frames(os.path.join(folder, video_file), train_output, target_fps=target_fps)\n\n# Process videos sequentially\nprocess_videos(\n    [\"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\"],\n    output_paths[\"train_violence\"],\n    output_paths[\"val_violence\"],\n    split_for_val=True\n)\nprocess_videos(\n    [\"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\"],\n    output_paths[\"train_nonviolence\"],\n    output_paths[\"val_nonviolence\"],\n    split_for_val=True\n)\n\nprint(\"YOLO-based bounding box extraction completed.\")\n\n# Extract features from cropped images sequentially\ntrain_violence_features = extract_features_from_images(output_paths[\"train_violence\"])\ntrain_nonviolence_features = extract_features_from_images(output_paths[\"train_nonviolence\"])\nval_violence_features = extract_features_from_images(output_paths[\"val_violence\"])\nval_nonviolence_features = extract_features_from_images(output_paths[\"val_nonviolence\"])\n\nprint(\"Feature extraction completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:45:34.455166Z","iopub.execute_input":"2025-01-01T16:45:34.455504Z","iopub.status.idle":"2025-01-01T16:45:44.146333Z","shell.execute_reply.started":"2025-01-01T16:45:34.455472Z","shell.execute_reply":"2025-01-01T16:45:44.144979Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nProcessing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   0%|          | 0/100 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 1 person, 2 cars, 9.0ms\nSpeed: 9.6ms preprocess, 9.0ms inference, 153.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.5ms\nSpeed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 7 persons, 2 cars, 7.5ms\nSpeed: 3.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 6 persons, 7.4ms\nSpeed: 2.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   1%|          | 1/100 [00:00<01:33,  1.05it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 1 person, 7.8ms\nSpeed: 2.8ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 car, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 (no detections), 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 4 persons, 2 bicycles, 7.5ms\nSpeed: 1.9ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   2%|▏         | 2/100 [00:01<00:44,  2.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 3 persons, 53.5ms\nSpeed: 1.9ms preprocess, 53.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 car, 7.8ms\nSpeed: 1.6ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 1 car, 7.6ms\nSpeed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 1 car, 1 backpack, 7.9ms\nSpeed: 1.9ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 1 car, 7.7ms\nSpeed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   3%|▎         | 3/100 [00:01<00:39,  2.49it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 14 persons, 3 cars, 1 truck, 8.0ms\nSpeed: 2.0ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 7 persons, 2 cars, 1 stop sign, 8.4ms\nSpeed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 5 cars, 1 bus, 1 truck, 1 backpack, 7.9ms\nSpeed: 2.0ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 8 persons, 2 cars, 9.5ms\nSpeed: 1.9ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 car, 8.3ms\nSpeed: 2.0ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 4 cars, 1 bus, 1 train, 1 truck, 7.9ms\nSpeed: 1.9ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   4%|▍         | 4/100 [00:01<00:39,  2.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 1 person, 8.2ms\nSpeed: 3.0ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 3 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 4 persons, 8.9ms\nSpeed: 2.8ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 9.6ms\nSpeed: 2.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 7.4ms\nSpeed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   5%|▌         | 5/100 [00:01<00:28,  3.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 2 persons, 8.1ms\nSpeed: 3.0ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 3 persons, 1 cup, 7.4ms\nSpeed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.4ms\nSpeed: 2.9ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 3 persons, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   6%|▌         | 6/100 [00:02<00:22,  4.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 10 persons, 1 cell phone, 7.4ms\nSpeed: 2.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 5 persons, 8.9ms\nSpeed: 2.7ms preprocess, 8.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 9 persons, 7.5ms\nSpeed: 1.9ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 5 persons, 7.4ms\nSpeed: 1.9ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 7.4ms\nSpeed: 2.6ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 4 persons, 1 backpack, 1 bottle, 7.4ms\nSpeed: 2.0ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 1 backpack, 7.5ms\nSpeed: 1.8ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 1 book, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 5 persons, 7.5ms\nSpeed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 5 persons, 7.5ms\nSpeed: 1.9ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   8%|▊         | 8/100 [00:02<00:15,  5.91it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x384 1 person, 39.6ms\nSpeed: 2.0ms preprocess, 39.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 1 person, 7.7ms\nSpeed: 2.0ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 1 person, 7.7ms\nSpeed: 1.4ms preprocess, 7.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 2 persons, 7.3ms\nSpeed: 1.4ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 4 persons, 7.4ms\nSpeed: 1.3ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:   9%|▉         | 9/100 [00:02<00:14,  6.13it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 2 persons, 1 car, 2 trucks, 1 handbag, 8.1ms\nSpeed: 2.1ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 car, 2 trucks, 7.5ms\nSpeed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 car, 1 bus, 1 truck, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 2 cars, 2 trucks, 7.5ms\nSpeed: 1.9ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 car, 2 trucks, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 384x640 (no detections), 8.5ms\nSpeed: 1.6ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 7.5ms\nSpeed: 1.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 8.3ms\nSpeed: 1.4ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 car, 8.1ms\nSpeed: 1.4ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 (no detections), 7.5ms\nSpeed: 1.2ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 car, 7.8ms\nSpeed: 1.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  11%|█         | 11/100 [00:02<00:13,  6.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 2 persons, 8.0ms\nSpeed: 2.3ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 (no detections), 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 3 persons, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  12%|█▏        | 12/100 [00:02<00:12,  7.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 4 persons, 1 banana, 8.6ms\nSpeed: 2.0ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 persons, 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 1 umbrella, 8.8ms\nSpeed: 1.9ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 cat, 7.9ms\nSpeed: 2.0ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 1 handbag, 8.4ms\nSpeed: 2.0ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 persons, 1 suitcase, 8.9ms\nSpeed: 2.3ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  13%|█▎        | 13/100 [00:03<00:16,  5.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 (no detections), 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 7.4ms\nSpeed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.5ms\nSpeed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 4 persons, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 toilet, 7.5ms\nSpeed: 2.6ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 576x640 1 person, 40.2ms\nSpeed: 3.1ms preprocess, 40.2ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 640)\n\n0: 576x640 5 persons, 1 suitcase, 6.6ms\nSpeed: 3.2ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n\n0: 576x640 4 persons, 6.7ms\nSpeed: 3.1ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n\n0: 576x640 (no detections), 6.8ms\nSpeed: 2.4ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 576, 640)\n\n0: 576x640 3 persons, 6.6ms\nSpeed: 2.3ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  15%|█▌        | 15/100 [00:03<00:15,  5.62it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 (no detections), 8.0ms\nSpeed: 2.1ms preprocess, 8.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.3ms\nSpeed: 2.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 7.3ms\nSpeed: 2.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 (no detections), 7.5ms\nSpeed: 2.1ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 3 persons, 1 surfboard, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 7.4ms\nSpeed: 2.0ms preprocess, 7.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  16%|█▌        | 16/100 [00:03<00:14,  5.94it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x384 3 persons, 8.2ms\nSpeed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 1 person, 1 chair, 7.3ms\nSpeed: 1.4ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 2 persons, 7.9ms\nSpeed: 1.9ms preprocess, 7.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 2 persons, 7.3ms\nSpeed: 1.4ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 2 persons, 1 suitcase, 7.4ms\nSpeed: 1.4ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  17%|█▋        | 17/100 [00:03<00:12,  6.42it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 4 persons, 8.1ms\nSpeed: 1.2ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 8.1ms\nSpeed: 1.4ms preprocess, 8.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 persons, 1 car, 1 umbrella, 8.4ms\nSpeed: 1.2ms preprocess, 8.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 persons, 1 car, 7.5ms\nSpeed: 1.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 9 persons, 3 cars, 7.7ms\nSpeed: 0.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 5 persons, 2 cars, 7.9ms\nSpeed: 1.2ms preprocess, 7.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  18%|█▊        | 18/100 [00:03<00:13,  5.93it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 2 persons, 1 car, 7.9ms\nSpeed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 persons, 8.6ms\nSpeed: 2.2ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 8.7ms\nSpeed: 2.1ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 persons, 7.9ms\nSpeed: 2.1ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 8.1ms\nSpeed: 2.1ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 persons, 1 baseball bat, 8.4ms\nSpeed: 2.0ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  19%|█▉        | 19/100 [00:04<00:18,  4.28it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x384 1 person, 1 baseball glove, 8.4ms\nSpeed: 2.2ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 3 persons, 1 car, 7.4ms\nSpeed: 1.3ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 2 persons, 1 car, 7.7ms\nSpeed: 1.3ms preprocess, 7.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 4 persons, 1 car, 7.3ms\nSpeed: 1.4ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n\n0: 640x384 1 person, 7.3ms\nSpeed: 1.4ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  20%|██        | 20/100 [00:04<00:15,  5.02it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 1 person, 8.2ms\nSpeed: 1.7ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 1 car, 8.4ms\nSpeed: 1.9ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 1 person, 8.6ms\nSpeed: 2.0ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 persons, 7.9ms\nSpeed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 2 persons, 8.1ms\nSpeed: 1.7ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  21%|██        | 21/100 [00:04<00:17,  4.40it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 512x640 1 person, 40.8ms\nSpeed: 2.6ms preprocess, 40.8ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n\n0: 512x640 2 persons, 1 suitcase, 6.7ms\nSpeed: 1.9ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n\n0: 512x640 2 persons, 6.6ms\nSpeed: 1.9ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n\n0: 512x640 2 persons, 6.7ms\nSpeed: 2.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n\n0: 512x640 2 persons, 6.7ms\nSpeed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  22%|██▏       | 22/100 [00:04<00:16,  4.83it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 1 backpack, 8.1ms\nSpeed: 2.2ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.5ms\nSpeed: 2.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.3ms\nSpeed: 2.1ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 bottle, 1 bowl, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 bowl, 7.5ms\nSpeed: 2.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  23%|██▎       | 23/100 [00:04<00:14,  5.44it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x352 (no detections), 39.2ms\nSpeed: 1.3ms preprocess, 39.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 352)\n\n0: 640x352 1 fire hydrant, 7.1ms\nSpeed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n\n0: 640x352 1 fire hydrant, 6.9ms\nSpeed: 1.3ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n\n0: 640x352 1 person, 7.4ms\nSpeed: 1.3ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n\n0: 640x352 (no detections), 7.3ms\nSpeed: 1.9ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 352)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  24%|██▍       | 24/100 [00:05<00:13,  5.72it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 384x640 5 persons, 9.4ms\nSpeed: 3.1ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 chair, 10.3ms\nSpeed: 3.2ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 6 persons, 1 suitcase, 8.2ms\nSpeed: 2.9ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 4 persons, 1 train, 9.1ms\nSpeed: 3.2ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 3 persons, 8.7ms\nSpeed: 3.1ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  25%|██▌       | 25/100 [00:05<00:27,  2.75it/s]","output_type":"stream"},{"name":"stdout","text":"\n0: 640x640 4 persons, 8.2ms\nSpeed: 3.1ms preprocess, 8.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 bottle, 8.2ms\nSpeed: 2.0ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 (no detections), 7.4ms\nSpeed: 2.7ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 2 persons, 7.5ms\nSpeed: 2.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x640 1 person, 1 parking meter, 7.4ms\nSpeed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n\n0: 640x416 1 person, 2 skateboards, 1 toilet, 1 sink, 40.7ms\nSpeed: 2.7ms preprocess, 40.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n\n0: 640x416 2 persons, 1 cell phone, 6.4ms\nSpeed: 2.4ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n","output_type":"stream"},{"name":"stderr","text":"Processing validation videos from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence:  26%|██▌       | 26/100 [00:06<00:17,  4.23it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m             extract_yolo_frames(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, video_file), train_output, target_fps\u001b[38;5;241m=\u001b[39mtarget_fps)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Process videos sequentially\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[43mprocess_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_violence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_violence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_for_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m process_videos(\n\u001b[1;32m    112\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    113\u001b[0m     output_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_nonviolence\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    114\u001b[0m     output_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_nonviolence\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    115\u001b[0m     split_for_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLO-based bounding box extraction completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[3], line 98\u001b[0m, in \u001b[0;36mprocess_videos\u001b[0;34m(video_paths, train_output, val_output, split_for_val, target_fps)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Extract YOLO bounding boxes for validation videos\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m tqdm(val_videos, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing validation videos from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mextract_yolo_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_fps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_fps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Extract YOLO bounding boxes for training videos\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m tqdm(train_videos, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training videos from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n","Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mextract_yolo_frames\u001b[0;34m(video_path, output_folder, target_fps)\u001b[0m\n\u001b[1;32m     49\u001b[0m frame_count, saved_frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m---> 52\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Paths to the directories containing extracted frames for training and validation\ntrain_dir = \"/kaggle/working/extracted_frames/train\"\nval_dir = \"/kaggle/working/extracted_frames/val\"\n\n# Parameters\nIMG_SIZE = (224, 224)  # Target image size for resizing\nBATCH_SIZE = 32  # Number of images per batch during training/validation\n\n# 1. Data Augmentation and Preprocessing\n# Training Data Augmentation (train_datagen)\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,  # Normalize pixel values from [0, 255] to [0, 1]\n    rotation_range=20,  # Random rotation within ±20 degrees\n    width_shift_range=0.2,  # Random horizontal shift (20% of image width)\n    height_shift_range=0.2,  # Random vertical shift (20% of image height)\n    shear_range=0.2,  # Random shearing transformations\n    zoom_range=0.2,  # Random zoom (in or out) by up to 20%\n    horizontal_flip=True,  # Random horizontal flip\n    brightness_range=[0.8, 1.2],  # Randomly adjust image brightness\n    fill_mode='nearest'  # Handle pixels outside image boundary using nearest pixel value\n)\n\n# Validation Data Preprocessing (val_datagen)\nval_datagen = ImageDataGenerator(\n    rescale=1.0/255  # Only rescale pixel values for validation (no augmentation)\n)\n\n# 2. Data Generators\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=IMG_SIZE,  # Resize images to the target size\n    batch_size=BATCH_SIZE,  # Group images into batches of size BATCH_SIZE\n    class_mode='binary',  # Binary classification (two classes: 0 or 1)\n    shuffle=True  # Shuffle the data for training to ensure randomness\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=IMG_SIZE,  # Resize images to the target size\n    batch_size=BATCH_SIZE,  # Group images into batches of size BATCH_SIZE\n    class_mode='binary',  # Binary classification for validation as well\n    shuffle=False  # Do not shuffle for validation, keeping consistent ordering\n)\n\n# These generators can now be used in a model's fit method for training and validation.\n# Example: cnn_model.fit(train_generator, validation_data=val_generator, epochs=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:45:52.872667Z","iopub.execute_input":"2025-01-01T16:45:52.873528Z","iopub.status.idle":"2025-01-01T16:45:54.289903Z","shell.execute_reply.started":"2025-01-01T16:45:52.873491Z","shell.execute_reply":"2025-01-01T16:45:54.289194Z"}},"outputs":[{"name":"stdout","text":"Found 74316 images belonging to 2 classes.\nFound 36154 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\n\n# Specify the path to the main directory\nmain_dir = '/kaggle/working/extracted_frames'\n\n# Dictionary to store file counts\nfile_counts = {}\n\n# Walk through each subdirectory and count files\nfor root, dirs, files in os.walk(main_dir):\n    # Only count files, skip directories\n    file_counts[root] = len(files)\n\n# Print the file counts in each folder\nfor folder, count in file_counts.items():\n    print(f\"{folder}: {count} files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:46:00.824343Z","iopub.execute_input":"2025-01-01T16:46:00.825061Z","iopub.status.idle":"2025-01-01T16:46:00.918419Z","shell.execute_reply.started":"2025-01-01T16:46:00.825025Z","shell.execute_reply":"2025-01-01T16:46:00.917505Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/extracted_frames: 0 files\n/kaggle/working/extracted_frames/train: 0 files\n/kaggle/working/extracted_frames/train/nonviolence: 32940 files\n/kaggle/working/extracted_frames/train/violence: 41376 files\n/kaggle/working/extracted_frames/val: 0 files\n/kaggle/working/extracted_frames/val/nonviolence: 14523 files\n/kaggle/working/extracted_frames/val/violence: 21631 files\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nfrom tqdm import tqdm\n\n# Image transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Data loaders\ntrain_dataset = datasets.ImageFolder('/kaggle/working/extracted_frames/train', transform=transform)\nval_dataset = datasets.ImageFolder('/kaggle/working/extracted_frames/val', transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n\n# Training loop\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in tqdm(loader):\n        images, labels = images.to(device), labels.to(device).float()\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels.unsqueeze(1))\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        predicted = (outputs > 0.5).float()\n        correct += (predicted == labels.unsqueeze(1)).sum().item()\n        total += labels.size(0)\n    \n    return running_loss/len(loader), correct/total\n\n# Start training\nnum_epochs = 10\nmodel.train()\n\nfor epoch in range(num_epochs):\n    loss, acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    print(f'Epoch {epoch+1}: Loss = {loss:.4f}, Acc = {acc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T16:50:28.650300Z","iopub.execute_input":"2025-01-01T16:50:28.650946Z","iopub.status.idle":"2025-01-01T17:12:18.836915Z","shell.execute_reply.started":"2025-01-01T16:50:28.650909Z","shell.execute_reply":"2025-01-01T17:12:18.835854Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2323/2323 [02:08<00:00, 18.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 0.4085, Acc = 0.8162\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:09<00:00, 18.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss = 0.3947, Acc = 0.8263\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:16<00:00, 17.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss = 0.3916, Acc = 0.8300\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:09<00:00, 17.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss = 0.3861, Acc = 0.8305\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:16<00:00, 17.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss = 0.3825, Acc = 0.8336\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:09<00:00, 17.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss = 0.3801, Acc = 0.8359\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:06<00:00, 18.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss = 0.3780, Acc = 0.8376\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:11<00:00, 17.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss = 0.3763, Acc = 0.8368\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:10<00:00, 17.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss = 0.3703, Acc = 0.8408\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:12<00:00, 17.59it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss = 0.3700, Acc = 0.8413\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'Total trainable parameters: {count_parameters(model):,}')\nprint(\"\\nModel Architecture:\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:12:49.510147Z","iopub.execute_input":"2025-01-01T17:12:49.511064Z","iopub.status.idle":"2025-01-01T17:12:49.521118Z","shell.execute_reply.started":"2025-01-01T17:12:49.511022Z","shell.execute_reply":"2025-01-01T17:12:49.520138Z"}},"outputs":[{"name":"stdout","text":"Total trainable parameters: 3,659,017\n\nModel Architecture:\nMobileNetClassifier(\n  (base_model): MobileNetV3(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (2): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (3): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (4): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (5): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (6): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (7): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (8): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (9): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (10): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (11): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (12): Conv2dNormActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n    (classifier): Sequential(\n      (0): Linear(in_features=576, out_features=1024, bias=True)\n      (1): Hardswish()\n      (2): Dropout(p=0.2, inplace=True)\n      (3): Linear(in_features=1024, out_features=1000, bias=True)\n    )\n  )\n  (features): Sequential(\n    (0): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (2): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (3): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (4): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (5): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (6): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (7): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (8): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (9): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (10): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (11): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (12): Conv2dNormActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n    )\n    (1): AdaptiveAvgPool2d(output_size=1)\n    (2): Flatten(start_dim=1, end_dim=-1)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=576, out_features=1024, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=512, out_features=1, bias=True)\n    (7): Sigmoid()\n  )\n)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport os\n\nclass ModelTrainer:\n    def __init__(self, model, train_loader, val_loader, device):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        \n        self.optimizer = Adam(model.parameters(), lr=1e-4)\n        self.criterion = torch.nn.BCELoss()\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-5)\n        \n    def train_epoch(self):\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in tqdm(self.train_loader):\n            inputs, labels = inputs.to(self.device), labels.to(self.device).float()\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, labels.unsqueeze(1))\n            \n            loss.backward()\n            self.optimizer.step()\n            \n            running_loss += loss.item()\n            predicted = (outputs > 0.5).float()\n            correct += (predicted == labels.unsqueeze(1)).sum().item()\n            total += labels.size(0)\n            \n        return running_loss/len(self.train_loader), correct/total\n    \n    def validate(self):\n        self.model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(self.val_loader):\n                inputs, labels = inputs.to(self.device), labels.to(self.device).float()\n                \n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels.unsqueeze(1))\n                \n                val_loss += loss.item()\n                predicted = (outputs > 0.5).float()\n                correct += (predicted == labels.unsqueeze(1)).sum().item()\n                total += labels.size(0)\n                \n        return val_loss/len(self.val_loader), correct/total\n\n    def train(self, epochs=15):\n        best_val_loss = float('inf')\n        \n        for epoch in range(epochs):\n            train_loss, train_acc = self.train_epoch()\n            val_loss, val_acc = self.validate()\n            \n            # Learning rate scheduling\n            self.scheduler.step(val_loss)\n            \n            # Model checkpoint\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'loss': val_loss,\n                }, 'best_model.pth')\n                print(f'Saved checkpoint at epoch {epoch+1}')\n            \n            print(f'Epoch {epoch+1}/{epochs}')\n            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n            print(f'Current LR: {self.optimizer.param_groups[0][\"lr\"]:.6f}\\n')\n\n# Usage\ntrainer = ModelTrainer(model, train_loader, val_loader, device)\ntrainer.train(epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:13:23.886527Z","iopub.execute_input":"2025-01-01T17:13:23.886874Z","iopub.status.idle":"2025-01-01T17:30:39.030274Z","shell.execute_reply.started":"2025-01-01T17:13:23.886846Z","shell.execute_reply":"2025-01-01T17:30:39.029243Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2323/2323 [02:10<00:00, 17.85it/s]\n100%|██████████| 1130/1130 [01:06<00:00, 16.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved checkpoint at epoch 1\nEpoch 1/5\nTrain Loss: 0.3034, Train Acc: 0.8732\nVal Loss: 0.4141, Val Acc: 0.8426\nCurrent LR: 0.000100\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:20<00:00, 16.52it/s]\n100%|██████████| 1130/1130 [01:11<00:00, 15.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5\nTrain Loss: 0.2908, Train Acc: 0.8793\nVal Loss: 0.4514, Val Acc: 0.8381\nCurrent LR: 0.000100\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:20<00:00, 16.56it/s]\n100%|██████████| 1130/1130 [01:10<00:00, 16.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5\nTrain Loss: 0.2820, Train Acc: 0.8829\nVal Loss: 0.4652, Val Acc: 0.8409\nCurrent LR: 0.000100\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:20<00:00, 16.48it/s]\n100%|██████████| 1130/1130 [01:06<00:00, 17.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5\nTrain Loss: 0.2772, Train Acc: 0.8858\nVal Loss: 0.4461, Val Acc: 0.8485\nCurrent LR: 0.000100\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2323/2323 [02:18<00:00, 16.78it/s]\n100%|██████████| 1130/1130 [01:09<00:00, 16.27it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5\nTrain Loss: 0.2737, Train Acc: 0.8874\nVal Loss: 0.5169, Val Acc: 0.8505\nCurrent LR: 0.000100\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\nfrom torch.optim import Adam\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport os\n\n# Define the MobileNetClassifier\nclass MobileNetClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Load the base model with pre-trained weights\n        self.base_model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n        \n        # Freeze the base model if needed\n        for param in self.base_model.parameters():\n            param.requires_grad = True\n        \n        # Modify final layers\n        self.features = nn.Sequential(\n            self.base_model.features,\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten()\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(576, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n\n# Create model and move to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MobileNetClassifier().to(device)\n\n# Setup optimizer and loss function\noptimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.01)  # L2 regularization\ncriterion = nn.BCELoss()\n\n# Define transformations for data preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # MobileNetV3 expects 224x224 images\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load dataset (adjusted to your directory structure)\ndata_dir = \"/kaggle/working/extracted_frames\"\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\nval_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Training loop with model saving\nnum_epochs = 5\nbest_val_loss = float('inf')  # Initialize to a very large value\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.float().to(device)\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n    \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.float().to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs.squeeze(), labels)\n            val_loss += loss.item()\n            \n            # Calculate accuracy\n            predicted = (outputs.squeeze() > 0.5).float()\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    \n    val_loss /= len(val_loader)\n    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n    \n    # Save the model if validation loss improves\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), \"/kaggle/working/best_model.pth\")\n        print(\"Saved best model.\")\n\n# Load the best model for further use or testing\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\"))\nmodel.to(device)\nprint(\"Loaded the best model.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:50:22.878328Z","iopub.execute_input":"2025-01-01T17:50:22.878679Z","iopub.status.idle":"2025-01-01T18:18:06.219937Z","shell.execute_reply.started":"2025-01-01T17:50:22.878648Z","shell.execute_reply":"2025-01-01T18:18:06.218988Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5], Loss: 0.4137\nValidation Loss: 0.7788, Accuracy: 52.82%\nSaved best model.\nEpoch [2/5], Loss: 0.3923\nValidation Loss: 0.8648, Accuracy: 63.38%\nEpoch [3/5], Loss: 0.3858\nValidation Loss: 0.4851, Accuracy: 77.69%\nSaved best model.\nEpoch [4/5], Loss: 0.3838\nValidation Loss: 0.4776, Accuracy: 78.90%\nSaved best model.\nEpoch [5/5], Loss: 0.3797\nValidation Loss: 0.5114, Accuracy: 75.84%\nLoaded the best model.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from torchinfo import torchinfo\n\n# Print model summary with sample input size\nsummary = torchinfo.summary(model, \n                          input_size=(1, 3, 224, 224),\n                          col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n                          depth=4)\nprint(summary)\n\n# If you don't have torchinfo installed, run:\n# !pip install torchinfo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T18:18:41.580268Z","iopub.execute_input":"2025-01-01T18:18:41.580623Z","iopub.status.idle":"2025-01-01T18:18:41.778874Z","shell.execute_reply.started":"2025-01-01T18:18:41.580592Z","shell.execute_reply":"2025-01-01T18:18:41.778026Z"}},"outputs":[{"name":"stdout","text":"====================================================================================================================================================================================\nLayer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n====================================================================================================================================================================================\nMobileNetClassifier                                     [1, 3, 224, 224]          [1, 1]                    1,615,848                 --                        --\n├─Sequential: 1-1                                       [1, 3, 224, 224]          [1, 576]                  --                        --                        --\n│    └─Sequential: 2-1                                  [1, 3, 224, 224]          [1, 576, 7, 7]            --                        --                        --\n│    │    └─Conv2dNormActivation: 3-1                   [1, 3, 224, 224]          [1, 16, 112, 112]         --                        --                        --\n│    │    │    └─Conv2d: 4-1                            [1, 3, 224, 224]          [1, 16, 112, 112]         432                       [3, 3]                    5,419,008\n│    │    │    └─BatchNorm2d: 4-2                       [1, 16, 112, 112]         [1, 16, 112, 112]         32                        --                        32\n│    │    │    └─Hardswish: 4-3                         [1, 16, 112, 112]         [1, 16, 112, 112]         --                        --                        --\n│    │    └─InvertedResidual: 3-2                       [1, 16, 112, 112]         [1, 16, 56, 56]           --                        --                        --\n│    │    │    └─Sequential: 4-4                        [1, 16, 112, 112]         [1, 16, 56, 56]           744                       --                        1,254,744\n│    │    └─InvertedResidual: 3-3                       [1, 16, 56, 56]           [1, 24, 28, 28]           --                        --                        --\n│    │    │    └─Sequential: 4-5                        [1, 16, 56, 56]           [1, 24, 28, 28]           3,864                     --                        5,475,792\n│    │    └─InvertedResidual: 3-4                       [1, 24, 28, 28]           [1, 24, 28, 28]           --                        --                        --\n│    │    │    └─Sequential: 4-6                        [1, 24, 28, 28]           [1, 24, 28, 28]           5,416                     --                        3,932,944\n│    │    └─InvertedResidual: 3-5                       [1, 24, 28, 28]           [1, 40, 14, 14]           --                        --                        --\n│    │    │    └─Sequential: 4-7                        [1, 24, 28, 28]           [1, 40, 14, 14]           13,736                    --                        3,034,568\n│    │    └─InvertedResidual: 3-6                       [1, 40, 14, 14]           [1, 40, 14, 14]           --                        --                        --\n│    │    │    └─Sequential: 4-8                        [1, 40, 14, 14]           [1, 40, 14, 14]           57,264                    --                        4,971,264\n│    │    └─InvertedResidual: 3-7                       [1, 40, 14, 14]           [1, 40, 14, 14]           --                        --                        --\n│    │    │    └─Sequential: 4-9                        [1, 40, 14, 14]           [1, 40, 14, 14]           57,264                    --                        4,971,264\n│    │    └─InvertedResidual: 3-8                       [1, 40, 14, 14]           [1, 48, 14, 14]           --                        --                        --\n│    │    │    └─Sequential: 4-10                       [1, 40, 14, 14]           [1, 48, 14, 14]           21,968                    --                        2,666,168\n│    │    └─InvertedResidual: 3-9                       [1, 48, 14, 14]           [1, 48, 14, 14]           --                        --                        --\n│    │    │    └─Sequential: 4-11                       [1, 48, 14, 14]           [1, 48, 14, 14]           29,800                    --                        3,427,480\n│    │    └─InvertedResidual: 3-10                      [1, 48, 14, 14]           [1, 96, 7, 7]             --                        --                        --\n│    │    │    └─Sequential: 4-12                       [1, 48, 14, 14]           [1, 96, 7, 7]             91,848                    --                        4,460,232\n│    │    └─InvertedResidual: 3-11                      [1, 96, 7, 7]             [1, 96, 7, 7]             --                        --                        --\n│    │    │    └─Sequential: 4-13                       [1, 96, 7, 7]             [1, 96, 7, 7]             294,096                   --                        6,293,712\n│    │    └─InvertedResidual: 3-12                      [1, 96, 7, 7]             [1, 96, 7, 7]             --                        --                        --\n│    │    │    └─Sequential: 4-14                       [1, 96, 7, 7]             [1, 96, 7, 7]             294,096                   --                        6,293,712\n│    │    └─Conv2dNormActivation: 3-13                  [1, 96, 7, 7]             [1, 576, 7, 7]            --                        --                        --\n│    │    │    └─Conv2d: 4-15                           [1, 96, 7, 7]             [1, 576, 7, 7]            55,296                    [1, 1]                    2,709,504\n│    │    │    └─BatchNorm2d: 4-16                      [1, 576, 7, 7]            [1, 576, 7, 7]            1,152                     --                        1,152\n│    │    │    └─Hardswish: 4-17                        [1, 576, 7, 7]            [1, 576, 7, 7]            --                        --                        --\n│    └─AdaptiveAvgPool2d: 2-2                           [1, 576, 7, 7]            [1, 576, 1, 1]            --                        --                        --\n│    └─Flatten: 2-3                                     [1, 576, 1, 1]            [1, 576]                  --                        --                        --\n├─Sequential: 1-2                                       [1, 576]                  [1, 1]                    --                        --                        --\n│    └─Linear: 2-4                                      [1, 576]                  [1, 1024]                 590,848                   --                        590,848\n│    └─ReLU: 2-5                                        [1, 1024]                 [1, 1024]                 --                        --                        --\n│    └─Dropout: 2-6                                     [1, 1024]                 [1, 1024]                 --                        --                        --\n│    └─Linear: 2-7                                      [1, 1024]                 [1, 512]                  524,800                   --                        524,800\n│    └─ReLU: 2-8                                        [1, 512]                  [1, 512]                  --                        --                        --\n│    └─Dropout: 2-9                                     [1, 512]                  [1, 512]                  --                        --                        --\n│    └─Linear: 2-10                                     [1, 512]                  [1, 1]                    513                       --                        513\n│    └─Sigmoid: 2-11                                    [1, 1]                    [1, 1]                    --                        --                        --\n====================================================================================================================================================================================\nTotal params: 3,659,017\nTrainable params: 3,659,017\nNon-trainable params: 0\nTotal mult-adds (M): 56.03\n====================================================================================================================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 22.64\nParams size (MB): 8.17\nEstimated Total Size (MB): 31.41\n====================================================================================================================================================================================\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n\n# Load the base model\nbase_model = MobileNetV3Small(\n    input_shape=(224, 224, 3),\n    include_top=False,  # Exclude the original classifier head\n    weights='imagenet'  # Use ImageNet weights\n)\n\n# Freeze the base model\nbase_model.trainable = True  # Set to False if you don't want to fine-tune\n\n# Add custom layers for binary classification\nmodel = Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    Dropout(0.5),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n\n# Compile the model\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-4),  # Fine-tuning with a lower learning rate\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Data preprocessing using ImageDataGenerator\ndata_dir = \"/kaggle/working/extracted_frames\"\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0 / 255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1.0 / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    directory=f\"{data_dir}/train\",\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    directory=f\"{data_dir}/val\",\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary'\n)\n\n# Define callbacks\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-5,\n    verbose=1\n)\n\ncheckpoint = ModelCheckpoint(\n    filepath='best_model.keras',\n    monitor='val_loss',\n    save_best_only=True,\n    mode='min',\n    verbose=1\n)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=5,\n    callbacks=[checkpoint, reduce_lr]\n)\n\n# Load the best model\nmodel = tf.keras.models.load_model('best_model.keras')\nprint(\"Best model loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T18:24:48.473441Z","iopub.execute_input":"2025-01-01T18:24:48.474053Z","iopub.status.idle":"2025-01-01T19:30:39.166121Z","shell.execute_reply.started":"2025-01-01T18:24:48.474015Z","shell.execute_reply":"2025-01-01T19:30:39.165205Z"}},"outputs":[{"name":"stdout","text":"Found 74316 images belonging to 2 classes.\nFound 36154 images belonging to 2 classes.\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1735755916.074809   47966 service.cc:145] XLA service 0x7ac728003f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1735755916.074870   47966 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   1/2323\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26:28:00\u001b[0m 41s/step - accuracy: 0.5000 - loss: 0.7422","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1735755936.047936   47966 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2322/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.7578 - loss: 0.4847\nEpoch 1: val_loss improved from inf to 0.72619, saving model to best_model.keras\n\u001b[1m2323/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m856s\u001b[0m 351ms/step - accuracy: 0.7579 - loss: 0.4846 - val_accuracy: 0.5142 - val_loss: 0.7262 - learning_rate: 1.0000e-04\nEpoch 2/5\n\u001b[1m2322/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.8930 - loss: 0.2496\nEpoch 2: val_loss did not improve from 0.72619\n\u001b[1m2323/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 330ms/step - accuracy: 0.8930 - loss: 0.2496 - val_accuracy: 0.4111 - val_loss: 1.9777 - learning_rate: 1.0000e-04\nEpoch 3/5\n\u001b[1m2322/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.9198 - loss: 0.1940\nEpoch 3: val_loss did not improve from 0.72619\n\u001b[1m2323/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m775s\u001b[0m 333ms/step - accuracy: 0.9198 - loss: 0.1940 - val_accuracy: 0.5938 - val_loss: 1.7396 - learning_rate: 1.0000e-04\nEpoch 4/5\n\u001b[1m2322/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.9373 - loss: 0.1577\nEpoch 4: val_loss did not improve from 0.72619\n\u001b[1m2323/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 331ms/step - accuracy: 0.9373 - loss: 0.1577 - val_accuracy: 0.5081 - val_loss: 2.4767 - learning_rate: 1.0000e-04\nEpoch 5/5\n\u001b[1m2322/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.9456 - loss: 0.1351\nEpoch 5: val_loss improved from 0.72619 to 0.49452, saving model to best_model.keras\n\u001b[1m2323/2323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 331ms/step - accuracy: 0.9456 - loss: 0.1351 - val_accuracy: 0.7924 - val_loss: 0.4945 - learning_rate: 1.0000e-04\nBest model loaded successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Save the model in .keras format\nmodel.save('trained_model.keras')\n\nprint(\"Model saved successfully in .keras format.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:33:03.946472Z","iopub.execute_input":"2025-01-01T19:33:03.946833Z","iopub.status.idle":"2025-01-01T19:33:04.791567Z","shell.execute_reply.started":"2025-01-01T19:33:03.946803Z","shell.execute_reply":"2025-01-01T19:33:04.790570Z"}},"outputs":[{"name":"stdout","text":"Model saved successfully in .keras format.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install tf2onnx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:31:39.551287Z","iopub.execute_input":"2025-01-01T19:31:39.551610Z","iopub.status.idle":"2025-01-01T19:31:48.365331Z","shell.execute_reply.started":"2025-01-01T19:31:39.551584Z","shell.execute_reply":"2025-01-01T19:31:48.364281Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting tf2onnx\n  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.26.4)\nRequirement already satisfied: onnx>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.17.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.16.0)\nRequirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (24.3.25)\nRequirement already satisfied: protobuf~=3.20 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (3.20.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (2024.8.30)\nDownloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tf2onnx\nSuccessfully installed tf2onnx-1.16.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import tensorflow as tf\nimport tf2onnx\nimport os\nimport numpy as np\n\ndef convert_keras_to_onnx(keras_model_path, onnx_model_path):\n    \"\"\"\n    Convert a Keras model to ONNX format\n    \n    Args:\n        keras_model_path (str): Path to the Keras model file (.keras)\n        onnx_model_path (str): Desired output path for the ONNX model\n        \n    Returns:\n        bool: True if conversion was successful, False otherwise\n    \"\"\"\n    try:\n        # Load the trained Keras model\n        model = tf.keras.models.load_model(keras_model_path)\n        \n        # Get input shape from the model's configuration\n        input_shape = model.layers[0].input_shape\n        if input_shape[0] is None:\n            # Replace None batch dimension with 1\n            input_shape = (1,) + input_shape[1:]\n        \n        # Create a dummy input to initialize the model\n        dummy_input = np.zeros(input_shape, dtype=np.float32)\n        \n        # Call the model once to initialize it\n        _ = model(dummy_input)\n        \n        # Save as SavedModel format first\n        temp_saved_model_path = \"temp_saved_model\"\n        tf.saved_model.save(model, temp_saved_model_path)\n        \n        # Convert to ONNX\n        output_path = onnx_model_path\n        command = f\"python -m tf2onnx.convert --saved-model {temp_saved_model_path} --output {output_path}\"\n        os.system(command)\n        \n        # Clean up temporary directory\n        if os.path.exists(temp_saved_model_path):\n            import shutil\n            shutil.rmtree(temp_saved_model_path)\n            \n        print(f\"Model successfully converted and saved to: {onnx_model_path}\")\n        return True\n        \n    except Exception as e:\n        print(f\"Error during conversion: {str(e)}\")\n        # Print more detailed error information\n        import traceback\n        print(traceback.format_exc())\n        return False\n\n# Example usage\nif __name__ == \"__main__\":\n    keras_model_path = 'trained_model.keras'\n    onnx_model_path = 'trained_model.onnx'\n    \n    success = convert_keras_to_onnx(keras_model_path, onnx_model_path)\n    \n    if success:\n        # Verify the ONNX model exists\n        if os.path.exists(onnx_model_path):\n            print(f\"ONNX model size: {os.path.getsize(onnx_model_path) / (1024*1024):.2f} MB\")\n        else:\n            print(\"Warning: ONNX file was not created successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T19:56:47.375070Z","iopub.execute_input":"2025-01-01T19:56:47.375724Z","iopub.status.idle":"2025-01-01T19:57:07.208838Z","shell.execute_reply.started":"2025-01-01T19:56:47.375685Z","shell.execute_reply":"2025-01-01T19:57:07.207813Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n2025-01-01 19:57:00,330 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n2025-01-01 19:57:02,449 - INFO - Signatures found in model: [serving_default].\n2025-01-01 19:57:02,449 - WARNING - '--signature_def' not specified, using first signature: serving_default\n2025-01-01 19:57:02,450 - INFO - Output names: ['output_0']\n2025-01-01 19:57:03,781 - INFO - Using tensorflow=2.16.1, onnx=1.17.0, tf2onnx=1.16.1/15c810\n2025-01-01 19:57:03,781 - INFO - Using opset <onnx, 15>\n2025-01-01 19:57:03,876 - INFO - Computed 0 values for constant folding\n2025-01-01 19:57:04,123 - INFO - Optimizing ONNX model\n2025-01-01 19:57:06,055 - INFO - After optimization: Add -41 (79->38), Const -108 (249->141), GlobalAveragePool +10 (0->10), Identity -2 (2->0), ReduceMean -10 (10->0), Reshape +7 (11->18), Squeeze +1 (0->1), Transpose -114 (115->1)\n2025-01-01 19:57:06,075 - INFO - \n2025-01-01 19:57:06,075 - INFO - Successfully converted TensorFlow model temp_saved_model to ONNX\n2025-01-01 19:57:06,075 - INFO - Model inputs: ['inputs']\n2025-01-01 19:57:06,075 - INFO - Model outputs: ['output_0']\n2025-01-01 19:57:06,075 - INFO - ONNX model is saved at trained_model.onnx\n","output_type":"stream"},{"name":"stdout","text":"Model successfully converted and saved to: trained_model.onnx\nONNX model size: 4.74 MB\n","output_type":"stream"}],"execution_count":18}]}